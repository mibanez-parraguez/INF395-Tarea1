{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 1 - Redes Neuronales y *Deep Learning* </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "\n",
    "**Integrantes**:\n",
    "* Felipe Santander (201104528-9)\n",
    "* Miguel Ibáñez (2990010-8)\n",
    "\n",
    "secciones:\n",
    "\n",
    "[1.](#primero) Red Neuronal *Feed Forward* para Detectar Exoplanetas  \n",
    "[2.](#segundo) *Deep Networks*  \n",
    "[3.](#tercero) Redes Convolucionales en Imágenes  \n",
    "[4.](#cuarto) CNN *vs* RNN Prediciendo el Ozono Atmosférico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Librerias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.regularizers import l1,l2\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"primero\"></a>\n",
    "## 1. Red Neuronal *Feed Forward* para Detectar Exoplanetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lectura de datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sets = pd.read_csv(\"./koi_sets_unb.csv\")\n",
    "mask_train = (df_sets[\"Set\"] == \"Train\").values\n",
    "mask_test = (df_sets[\"Set\"] == \"Test\").values\n",
    "df_labels = pd.read_csv(\"./koi_labels.csv\")\n",
    "df_X = pd.read_csv(\"./koi_light_curves_X.csv\")\n",
    "df_labels_train = df_labels[mask_train]\n",
    "df_labels_test = df_labels[mask_test]\n",
    "df_X_train = df_X[mask_train]\n",
    "df_X_test = df_X[mask_test]\n",
    "\n",
    "y_train = ((df_labels_train[\"NExScI Disposition\"]==\"CONFIRMED\")*1).values # vector categorico (1 o 0)\n",
    "y_test = ((df_labels_test[\"NExScI Disposition\"]==\"CONFIRMED\")*1).values\n",
    "df_X_train = df_X_train.reset_index(drop=True)\n",
    "df_X_test = df_X_test.reset_index(drop=True)\n",
    "nan_X_train = df_X_train.isna().sum() # Sumario de datos nulos\n",
    "nan_X_test = df_X_test.isna().sum()\n",
    "df_X_train.fillna(df_X_train.median(), inplace=True)\n",
    "df_X_test.fillna(df_X_test.median(), inplace=True)\n",
    "X_train = df_X_train.values[:,1:] # Quita encabezado de las columnas\n",
    "X_test = df_X_test.values[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Descripción data.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confirmados (clase 1): {}\\nno confirmados (cl 0): {}\".format(sum(y_train), len(y_train)-sum(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_X_train.columns[1:].shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "df_X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problema de transformar un vector $X$ en un valor categórico (0 o 1).\n",
    "\n",
    "- Los atributos están en distinta escala?\n",
    "- Muchos atributos (dimensiones)\n",
    "- No es simple determinar la importancia de cada atributo para formar un modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Escalamiento.**\n",
    "\n",
    "Ayuda a tener una varianza similar entre los distintos atributos. Un atributo con una varianza de ordenes de magnitud mayor al resto de los atributos, puede tener una influencia mayor que el resto en el modelo independiente de la importancia real del atributo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "scaler.fit(X_test) # ???\n",
    "X_test_scaled =  scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conjunto de validación, manteniendo desbalanceo\n",
    "\n",
    "Se extrae el 20% de elementos de cada clase (*confirmed* y *no confirmed*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjunto validacion\n",
    "n_cl1 =int(sum(y_train)*.2)\n",
    "n_cl0 =int((len(y_train)-sum(y_train))*.2)\n",
    "idx_cl1 = np.random.choice(np.arange(len(y_train))[y_train.astype(dtype=bool)], n_cl1, replace=False)\n",
    "idx_cl0 = np.random.choice(np.arange(len(y_train))[np.logical_not(y_train)], n_cl0, replace=False)\n",
    "X_val_scaled = np.append(X_train_scaled[idx_cl1], X_train_scaled[idx_cl0], axis=0)\n",
    "y_val = np.ones(n_cl1 + n_cl0)\n",
    "y_val[n_cl1:] = 0\n",
    "# Saca del conjunto de entrenamiento los elementos que ahora estan en validacion\n",
    "idx_aux = np.ones(len(y_train), dtype=bool)\n",
    "idx_aux[idx_cl1] = False\n",
    "idx_aux[idx_cl0] = False\n",
    "X_train_scaled = X_train_scaled[idx_aux] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) sigmoid vs relu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def plotModel(N, mid_activ, optim, batchsz=None, l1=None, l2=None, eps=100, title=None, text=None):\n",
    "    model = None\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20,8))\n",
    "    for i in range(N):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, input_dim=X_train_scaled.shape[1], \n",
    "                        kernel_initializer='uniform', activation=mid_activ, \n",
    "                        kernel_regularizer=l2, activity_regularizer=l1))\n",
    "        model.add(Dense(1, kernel_initializer='uniform', activation=\"sigmoid\", \n",
    "                        kernel_regularizer=l2, activity_regularizer=l1))\n",
    "        model.compile(optimizer=optim, loss='binary_crossentropy')\n",
    "        hist = model.fit(X_train_scaled, y_train, batch_size=batchsz, epochs=eps, verbose=0, validation_data=(X_val_scaled, y_val))\n",
    "        ax.plot(range(eps), hist.history[\"loss\"], 'o')\n",
    "        ax.plot(range(eps), hist.history[\"val_loss\"], 'x')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.text(0.85, 0.80, 'Activation: {}'.format(mid_activ), transform=ax.transAxes, fontsize=\"large\")\n",
    "    h = (Line2D([], [], lw=0, color='green', marker=\"o\", markersize=10), \n",
    "         Line2D([], [], lw=0, color='blue', marker='x', markersize=10))\n",
    "    l = (\"Train set\", \"Validation\")\n",
    "    ax.legend(h, l, loc='upper left', bbox_to_anchor=(0.84, 0.98), fontsize='x-large')\n",
    "    if(title):\n",
    "        ax.set_title(title, fontsize='xx-large')\n",
    "    else:\n",
    "        ax.set_title(\"Cross Entropy Loss vs epoch\", fontsize='xx-large')\n",
    "    if(text):\n",
    "        # INFO text => array [dict {x: coord_x, y: coord_y, txt: texto, kw: **kwargs}]\n",
    "        for tx in text:\n",
    "            ax.text(tx['x'], tx['y'], tx['txt'], transform=ax.transAxes, **tx[\"kw\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid\n",
    "model_sigmoid = plotModel(10, \"sigmoid\", SGD(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu\n",
    "model_relu = plotModel(10, \"relu\", SGD(lr=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrica f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f1_score(y_test,model_sigmoid.predict_classes(X_test_scaled),average='weighted')\n",
    "f1_score(y_test,model_relu.predict_classes(X_test_scaled),average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)  Tasa de aprendizaje**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lr = 20\n",
    "learn_rate = np.linspace(0,1,n_lr)\n",
    "\n",
    "for lrn in learn_rate:\n",
    "    txt = {'x': 0.8, 'y': 0.8, 'txt': \"Learning rate: {:.2g}\".format(lrn), 'kw': {}}\n",
    "    title = \"Loss vs epoch (w/Learning rate: {:.2g}\".format(lrn)\n",
    "    model_sigmoid = plotModel(10, \"sigmoid\", SGD(lr=lrn), title=title, text=txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e) _Progressive decay_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_decay = 10\n",
    "lear_decay = np.logspace(-6,0,n_decay)\n",
    "\n",
    "for lrd in lear_decay:\n",
    "    txt = [{'x': 0.8, 'y': 0.8, 'txt': \"Learning rate: 0.2\", 'kw': {}}, \n",
    "           {'x': 0.8, 'y': 0.75, 'txt': \"Learning rate: {:e}\".format(lrd), 'kw': {}}]\n",
    "    title = \"Loss vs epoch (w/ learning decay {:.4e})\".format(lrd)\n",
    "    model_sigmoid = plotModel(10, \"sigmoid\", SGD(lr=0.2, decay=lrd), title=title, text=txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lrd in lear_decay:\n",
    "    txt = [{'x': 0.8, 'y': 0.8, 'txt': \"Learning rate: 0.2\", 'kw': {}}, \n",
    "           {'x': 0.8, 'y': 0.75, 'txt': \"Learning rate: {:e}\".format(lrd), 'kw': {}}]\n",
    "    title = \"Loss vs epoch (w/ learning decay {:.4e})\".format(lrd)\n",
    "    model_relu = plotModel(10, \"relu\", SGD(lr=0.2, decay=lrd), title=title, text=txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f) Momentum clásico y momentum de Nesterov**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_space = 21\n",
    "momentums = np.linspace(0,1,split_space)\n",
    "# Momentum clásico\n",
    "print(\"Momentum clásico\\n================\")\n",
    "for mmt in momentums:\n",
    "    txt = [{'x': 0.85, 'y': 0.76, 'txt': \"Learning rate: 0.01\", 'kw': {}}, \n",
    "           {'x': 0.85, 'y': 0.72, 'txt': \"Momentum: {:.2g}\".format(mmt), 'kw': {}}, \n",
    "           {'x': 0.85, 'y': 0.68, 'txt': \"Nesterov: False\"}, 'kw': {}]\n",
    "    model_sigmoid = plotModel(10, \"sigmoid\", SGD(lr=0.01, momentum=mmt, nesterov=False), text=txt)\n",
    "    model_sigmoid = plotModel(10, \"relu\", SGD(lr=0.01, momentum=mmt, nesterov=False), text=txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum de nesterov\n",
    "print(\"Momentum de nesterov\\n=====================\")\n",
    "for mmt in momentums:\n",
    "    txt = [{'x': 0.85, 'y': 0.76, 'txt': \"Learning rate: 0.01\", 'kw': {}}, \n",
    "           {'x': 0.85, 'y': 0.72, 'txt': \"Momentum: {:.2g}\".format(mmt), 'kw': {}}, \n",
    "           {'x': 0.85, 'y': 0.68, 'txt': \"Nesterov: True\"}, 'kw': {}]\n",
    "    model_sigmoid = plotModel(10, \"sigmoid\", SGD(lr=0.01, momentum=mmt, nesterov=True), text=txt)\n",
    "    model_sigmoid = plotModel(10, \"relu\", SGD(lr=0.01, momentum=mmt, nesterov=True), text=txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g) Batch size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = 21\n",
    "batch_sizes = np.round(np.linspace(1,X_train_scaled.shape[0],n_batches)).astype(dtype=int)\n",
    "\n",
    "for bsz in batch_sizes:\n",
    "    txt = [{'x': 0.85, 'y': 0.76, 'txt': \"Learning rate: 0.01\", 'kw': {}},\n",
    "           {'x': 0.85, 'y': 0.72, 'txt': \"Batch size: {:n}\".format(bsz), 'kw': {'weight': 'bold'}}]\n",
    "    title = \"Loss vs epoch (batch_size: {:n})\".format(bsz)\n",
    "    model_sigmoid = plotModel(4, \"sigmoid\", SGD(lr=0.01), batchsz=bsz, title=title, text=txt)\n",
    "    #model_relu = plotModel(4, \"relu\", SGD(lr=0.01), batchsz=bsz, title=title, text=txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bsz in batch_sizes:\n",
    "    txt = [{'x': 0.85, 'y': 0.76, 'txt': \"Learning rate: 0.01\", 'kw': {}},\n",
    "           {'x': 0.85, 'y': 0.72, 'txt': \"Batch size: {:n}\".format(bsz), 'kw': {'weight': 'bold'}}]\n",
    "    title = \"Loss vs epoch (batch_size: {:n})\".format(bsz)\n",
    "    model_relu = plotModel(4, \"relu\", SGD(lr=0.01), batchsz=bsz, title=title, text=txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**h) Optimizers: Adam, RMSprop, Adagrad & Adadelta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad\n",
    "model_sigmoid = plotModel(10, \"sigmoid\", Adagrad(lr=0.01))\n",
    "model_relu = plotModel(10, \"relu\", Adagrad(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adadelta\n",
    "model_sigmoid = plotModel(10, \"sigmoid\", Adadelta(lr=0.01))\n",
    "model_relu = plotModel(10, \"relu\", Adadelta(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam\n",
    "model_sigmoid = plotModel(10, \"sigmoid\", Adam(lr=0.01))\n",
    "model_relu = plotModel(10, \"relu\", Adam(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSprop\n",
    "model_sigmoid = plotModel(10, \"sigmoid\", RMSprop(lr=0.01))\n",
    "model_relu = plotModel(10, \"relu\", RMSprop(lr=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i) Regularizadores clásicos $l_1$ y $l_2$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO rango de los parametros?\n",
    "\n",
    "n_par = 10\n",
    "l12_param = np.logspace(-6,0,n_par)\n",
    "\n",
    "for p in l12_param:\n",
    "    txt = [{'x': 0.85, 'y': 0.76, 'txt': \"L1: {:.4e}\".format(p), 'kw': {'weight': 'bold'}}]\n",
    "    title = \"Loss vs epoch (w/ Activity Regularizer, L1 = {:.4e})\".format(p)\n",
    "    model_sigmoid = plotModel2(10, \"sigmoid\", SGD(lr=0.01), l1=l1(p), title=title, text=txt)\n",
    "    model_relu = plotModel2(10, \"relu\", SGD(lr=0.01), l1=l1(p), title=title, text=txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in l12_param:\n",
    "    txt = [{'x': 0.85, 'y': 0.76, 'txt': \"L2: {:.4e}\".format(p), 'kw': {'weight': 'bold'}}]\n",
    "    title = \"Loss vs epoch (w/ Kerner regularizer, L2 = {:.4e})\".format(p)\n",
    "    model_sigmoid = plotModel2(10, \"sigmoid\", SGD(lr=0.01), l2=l2(p), title=title, text=txt)\n",
    "    model_relu = plotModel2(10, \"relu\", SGD(lr=0.01), l1=l1(p), title=title, text=txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**j)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"segundo\"></a>\n",
    "## 2. *Deep Networks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tercero\"></a>\n",
    "## 3. Redes Convolucionales en Imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cuarto\"></a>\n",
    "## 4. CNN *vs* RNN Prediciendo el Ozono Atmosférico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
